{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec31ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.181.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-dotenv pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f62bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: users.csv\n",
      "File URI: https://generativelanguage.googleapis.com/v1beta/files/us7e1ggb454x\n",
      "Here are the answers based on the provided dataset:\n",
      "\n",
      "**1. How many users are in the dataset?**\n",
      "\n",
      "There are 11 users in the dataset.\n",
      "\n",
      "**2. What is the most common role?**\n",
      "\n",
      "The most common role is \"Operator\", with 7 users.\n",
      "\n",
      "**3. When was the most recent login?**\n",
      "\n",
      "The most recent login was on 2025-09-12T20:00:00Z.\n",
      "\n",
      "**4. When was the earliest login?**\n",
      "\n",
      "The earliest login was on 2025-09-12T11:00:00Z.\n",
      "\n",
      "**5.  What is the userId of the user with the role \"Admin\"?**\n",
      "\n",
      "The userId of the user with the role \"Admin\" is user002.\n",
      "\n",
      "**6. How many Maintenance Managers are there?**\n",
      "\n",
      "There are 2 Maintenance Managers.\n",
      "\n",
      "**7. What is the average time (in days) between account creation (`createdAt`) and last login (`lastLogin`)?**\n",
      "\n",
      "To calculate this accurately, we need to convert the date and time strings to a numerical representation (e.g., Unix timestamps or datetime objects in a programming language like Python).  A simple subtraction of the strings won't work directly due to date formatting. I can't perform this calculation directly here without a programming environment.  However, the process would involve:\n",
      "\n",
      "* Converting each `createdAt` and `lastLogin` to a numerical timestamp.\n",
      "* Subtracting the `createdAt` timestamp from the `lastLogin` timestamp for each user.\n",
      "* Summing these differences.\n",
      "* Dividing the sum by the number of users (11) to get the average difference in seconds.\n",
      "* Converting the average difference from seconds to days.\n",
      "\n",
      "\n",
      "**8. Which user has the longest time between `createdAt` and `lastLogin`?**\n",
      "\n",
      "Similar to question 7, this requires converting the dates to timestamps and calculating the differences.  The user with the largest difference would then be identified.  I can't provide this answer directly without a coding environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Upload CSV correctly (force mime_type as text/csv)\n",
    "file = genai.upload_file(\n",
    "    path=\"ArtificialData/users.csv\",\n",
    "    mime_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"Uploaded file:\", file.display_name)\n",
    "print(\"File URI:\", file.uri)\n",
    "\n",
    "# Query Gemini with the uploaded file\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"Answer questions based on this dataset of users:\",\n",
    "    file\n",
    "])\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bd8707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supervisor user ID is `user003` and the admin user ID is `user002`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"User ID of supervisor and admin users in the dataset:\",\n",
    "    file\n",
    "])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7193955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing file references...\n",
      "Successfully loaded 6 existing uploaded files\n",
      "\n",
      "============================================================\n",
      "RUNNING ANALYSIS...\n",
      "============================================================\n",
      "\n",
      "STRUCTURED JSON OUTPUT:\n",
      "{\n",
      "  \"csv_files_analyzed\": [\n",
      "    \"train_data.csv\",\n",
      "    \"cleaning_schedule.csv\",\n",
      "    \"train_status.csv\"\n",
      "  ],\n",
      "  \"question\": \"Which Train is due cleaning and which Train is out of service in the dataset?\",\n",
      "  \"answer\": \"Trains due for cleaning: TS004, TS005, TS007, TS008, TS013, TS014, TS015, TS021, TS022, TS025. Trains out of service: TS010, TS020\",\n",
      "  \"question_type\": \"filtering\",\n",
      "  \"summary\": {\n",
      "    \"total_files_analyzed\": 3,\n",
      "    \"total_trains_due_cleaning\": 10,\n",
      "    \"total_trains_out_of_service\": 2\n",
      "  },\n",
      "  \"trains_due_cleaning\": [\n",
      "    {\n",
      "      \"train_id\": \"TS004\",\n",
      "      \"train_name\": \"Hyundai\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS005\",\n",
      "      \"train_name\": \"Paytm\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS007\",\n",
      "      \"train_name\": \"Pepsi\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS008\",\n",
      "      \"train_name\": \"Hyundai\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS013\",\n",
      "      \"train_name\": \"ICICI\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS014\",\n",
      "      \"train_name\": \"Pepsi\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS015\",\n",
      "      \"train_name\": \"CocaCola\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS021\",\n",
      "      \"train_name\": \"Mahindra\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS022\",\n",
      "      \"train_name\": \"Airtel\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS025\",\n",
      "      \"train_name\": \"Flipkart\"\n",
      "    }\n",
      "  ],\n",
      "  \"trains_out_of_service\": [\n",
      "    {\n",
      "      \"train_id\": \"TS010\",\n",
      "      \"train_name\": \"McDonald\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS020\",\n",
      "      \"train_name\": \"Reliance\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Files remain uploaded for future use!\n",
      "Run this script again to reuse the same files.\n",
      "To clean up files, uncomment the line below:\n",
      "============================================================\n",
      "\n",
      "Currently uploaded files (32):\n",
      "- users.csv (ID: files/xhucpiaub75x)\n",
      "- trainsets.csv (ID: files/0oj7763bqrcb)\n",
      "- stabling_bays.csv (ID: files/7bx2xmg4krv1)\n",
      "- health_and_maintenance.csv (ID: files/uvxzfk7nf3kw)\n",
      "- cleaning_slots.csv (ID: files/aj4w642cgw1o)\n",
      "- branding_priorities.csv (ID: files/nbhtf7qybgb7)\n",
      "- users.csv (ID: files/imy0vw928di1)\n",
      "- trainsets.csv (ID: files/clf1s362h6at)\n",
      "- stabling_bays.csv (ID: files/t2n7dk9yxz5s)\n",
      "- health_and_maintenance.csv (ID: files/4g1iuy695tmh)\n",
      "- cleaning_slots.csv (ID: files/eacjz9nxys1m)\n",
      "- branding_priorities.csv (ID: files/sbpujgi0t9dv)\n",
      "- users.csv (ID: files/h4nea4p6cphe)\n",
      "- trainsets.csv (ID: files/9jpyj78xmd0x)\n",
      "- stabling_bays.csv (ID: files/3tfne1fgvwin)\n",
      "- health_and_maintenance.csv (ID: files/assb7pl4htmj)\n",
      "- cleaning_slots.csv (ID: files/41ck1raygkib)\n",
      "- branding_priorities.csv (ID: files/1suz4rzsxpw1)\n",
      "- users.csv (ID: files/po14yzslsb35)\n",
      "- trainsets.csv (ID: files/seuam4hyahch)\n",
      "- stabling_bays.csv (ID: files/mb2klwf3aliw)\n",
      "- health_and_maintenance.csv (ID: files/7gu0yor59jw2)\n",
      "- cleaning_slots.csv (ID: files/c1clwv6xy00f)\n",
      "- branding_priorities.csv (ID: files/dmn3vr17yn0b)\n",
      "- users.csv (ID: files/bs3a1jww9c9l)\n",
      "- trainsets.csv (ID: files/t0qqgo192g7b)\n",
      "- stabling_bays.csv (ID: files/143kntz9kl5b)\n",
      "- health_and_maintenance.csv (ID: files/vecw2xt4y4yi)\n",
      "- cleaning_slots.csv (ID: files/0xdyo0hgqng0)\n",
      "- branding_priorities.csv (ID: files/p5x28gdxy1lo)\n",
      "- users.csv (ID: files/us7e1ggb454x)\n",
      "- users.csv (ID: files/bnlybjiywznf)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Define JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"csv_files_analyzed\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "        \"answer\": {\"type\": \"string\"},\n",
    "        \"question_type\": {\"type\": \"string\", \"enum\": [\"summary\", \"filtering\", \"aggregation\", \"transformation\", \"comparison\"]},\n",
    "        \"trains_due_cleaning\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"train_id\": {\"type\": \"string\"},\n",
    "                    \"train_name\": {\"type\": \"string\"},\n",
    "                    \"status\": {\"type\": \"string\"},\n",
    "                    \"last_cleaned\": {\"type\": \"string\"},\n",
    "                    \"source_file\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"trains_out_of_service\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"train_id\": {\"type\": \"string\"},\n",
    "                    \"train_name\": {\"type\": \"string\"},\n",
    "                    \"status\": {\"type\": \"string\"},\n",
    "                    \"reason\": {\"type\": \"string\"},\n",
    "                    \"source_file\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"total_trains_due_cleaning\": {\"type\": \"integer\"},\n",
    "                \"total_trains_out_of_service\": {\"type\": \"integer\"},\n",
    "                \"total_files_analyzed\": {\"type\": \"integer\"}\n",
    "            }\n",
    "        },\n",
    "        \"pandas_code\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"csv_files_analyzed\", \"question\", \"answer\", \"question_type\", \"summary\"]\n",
    "}\n",
    "\n",
    "def upload_csv_files_once(csv_folder=\"ArtificialData\", file_refs_path=\"uploaded_file_refs.json\"):\n",
    "    \"\"\"Upload CSV files once and save their references\"\"\"\n",
    "    \n",
    "    # Check if we already have uploaded file references\n",
    "    if os.path.exists(file_refs_path):\n",
    "        print(\"Loading existing file references...\")\n",
    "        with open(file_refs_path, 'r') as f:\n",
    "            file_data = json.load(f)\n",
    "        \n",
    "        # Verify files still exist on Google's servers\n",
    "        try:\n",
    "            files = []\n",
    "            for file_info in file_data:\n",
    "                file_obj = genai.get_file(file_info['name'])\n",
    "                files.append(file_obj)\n",
    "            print(f\"Successfully loaded {len(files)} existing uploaded files\")\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"Some files may have expired or been deleted: {e}\")\n",
    "            print(\"Re-uploading files...\")\n",
    "    \n",
    "    # Upload files for the first time or re-upload if needed\n",
    "    csv_files = [os.path.join(csv_folder, f) for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {csv_folder}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Uploading {len(csv_files)} CSV files...\")\n",
    "    files = []\n",
    "    file_refs = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            uploaded_file = genai.upload_file(path=csv_file, mime_type=\"text/csv\")\n",
    "            files.append(uploaded_file)\n",
    "            file_refs.append({\n",
    "                'name': uploaded_file.name,\n",
    "                'display_name': uploaded_file.display_name,\n",
    "                'local_path': csv_file\n",
    "            })\n",
    "            print(f\"Uploaded: {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {csv_file}: {e}\")\n",
    "    \n",
    "    # Save file references for future use\n",
    "    with open(file_refs_path, 'w') as f:\n",
    "        json.dump(file_refs, f, indent=2)\n",
    "    \n",
    "    print(f\"Successfully uploaded and saved references for {len(files)} files\")\n",
    "    return files\n",
    "\n",
    "def analyze_trains(files, question=\"Which Train is due cleaning and which Train is out of service in the dataset?\"):\n",
    "    \"\"\"Analyze the uploaded CSV files with the given question\"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the uploaded CSV datasets to answer: \"{question}\"\n",
    "\n",
    "    Please examine all the CSV files and provide a structured JSON response with the following information:\n",
    "    1. List train IDs/names that are due for cleaning (simple array of strings)\n",
    "    2. List train IDs/names that are out of service (simple array of strings)\n",
    "    3. Provide detailed information for each train in separate \"train_details\" section\n",
    "    4. Provide a summary count\n",
    "\n",
    "    Structure your response as follows:\n",
    "    - trains_due_cleaning: Simple array of train identifiers\n",
    "    - trains_out_of_service: Simple array of train identifiers  \n",
    "    - train_details: Separate section with detailed information for each train\n",
    "    - summary: Count totals\n",
    "\n",
    "    Please respond ONLY with valid JSON that matches the specified schema. Do not include any markdown formatting or additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            [prompt, *files],\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=schema\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        structured_output = json.loads(response.text)\n",
    "        return structured_output\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON response. Error: {e}\")\n",
    "        print(\"Raw response:\", response.text)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_uploaded_files():\n",
    "    \"\"\"List all currently uploaded files\"\"\"\n",
    "    try:\n",
    "        files = list(genai.list_files())\n",
    "        print(f\"\\nCurrently uploaded files ({len(files)}):\")\n",
    "        for file in files:\n",
    "            print(f\"- {file.display_name} (ID: {file.name})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "\n",
    "def cleanup_files(file_refs_path=\"uploaded_file_refs.json\"):\n",
    "    \"\"\"Clean up uploaded files and remove references\"\"\"\n",
    "    if os.path.exists(file_refs_path):\n",
    "        with open(file_refs_path, 'r') as f:\n",
    "            file_data = json.load(f)\n",
    "        \n",
    "        for file_info in file_data:\n",
    "            try:\n",
    "                genai.delete_file(file_info['name'])\n",
    "                print(f\"Deleted: {file_info['display_name']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not delete {file_info['display_name']}: {e}\")\n",
    "        \n",
    "        os.remove(file_refs_path)\n",
    "        print(\"Cleaned up file references\")\n",
    "    else:\n",
    "        print(\"No file references found to clean up\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Upload files once (or load existing references)\n",
    "    files = upload_csv_files_once()\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files available for analysis\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Now you can run multiple analyses without re-uploading\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING ANALYSIS...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analysis 1\n",
    "    result = analyze_trains(files)\n",
    "    if result:\n",
    "        print(\"\\nSTRUCTURED JSON OUTPUT:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Save result\n",
    "        with open(\"train_analysis_result.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "    \n",
    "    # You can run more analyses here without re-uploading:\n",
    "    # result2 = analyze_trains(files, \"What is the average age of trains in the dataset?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Files remain uploaded for future use!\")\n",
    "    print(\"Run this script again to reuse the same files.\")\n",
    "    print(\"To clean up files, uncomment the line below:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Uncomment to clean up files when done:\n",
    "    # cleanup_files()\n",
    "    \n",
    "    # List currently uploaded files\n",
    "    list_uploaded_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
