{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec31ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.181.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\kmrl-document-overload-automation\\.venv\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-dotenv pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe08a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: users.csv\n",
      "File URI: https://generativelanguage.googleapis.com/v1beta/files/bnlybjiywznf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Upload CSV file\n",
    "file = genai.upload_file(\"ArtificialData\\\\users.csv\")\n",
    "\n",
    "print(\"Uploaded file:\", file.display_name)\n",
    "print(\"File URI:\", file.uri)  # Use this to reference in prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f62bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: users.csv\n",
      "File URI: https://generativelanguage.googleapis.com/v1beta/files/us7e1ggb454x\n",
      "Here are the answers based on the provided dataset:\n",
      "\n",
      "**1. How many users are in the dataset?**\n",
      "\n",
      "There are 11 users in the dataset.\n",
      "\n",
      "**2. What is the most common role?**\n",
      "\n",
      "The most common role is \"Operator\", with 7 users.\n",
      "\n",
      "**3. When was the most recent login?**\n",
      "\n",
      "The most recent login was on 2025-09-12T20:00:00Z.\n",
      "\n",
      "**4. When was the earliest login?**\n",
      "\n",
      "The earliest login was on 2025-09-12T11:00:00Z.\n",
      "\n",
      "**5.  What is the userId of the user with the role \"Admin\"?**\n",
      "\n",
      "The userId of the user with the role \"Admin\" is user002.\n",
      "\n",
      "**6. How many Maintenance Managers are there?**\n",
      "\n",
      "There are 2 Maintenance Managers.\n",
      "\n",
      "**7. What is the average time (in days) between account creation (`createdAt`) and last login (`lastLogin`)?**\n",
      "\n",
      "To calculate this accurately, we need to convert the date and time strings to a numerical representation (e.g., Unix timestamps or datetime objects in a programming language like Python).  A simple subtraction of the strings won't work directly due to date formatting. I can't perform this calculation directly here without a programming environment.  However, the process would involve:\n",
      "\n",
      "* Converting each `createdAt` and `lastLogin` to a numerical timestamp.\n",
      "* Subtracting the `createdAt` timestamp from the `lastLogin` timestamp for each user.\n",
      "* Summing these differences.\n",
      "* Dividing the sum by the number of users (11) to get the average difference in seconds.\n",
      "* Converting the average difference from seconds to days.\n",
      "\n",
      "\n",
      "**8. Which user has the longest time between `createdAt` and `lastLogin`?**\n",
      "\n",
      "Similar to question 7, this requires converting the dates to timestamps and calculating the differences.  The user with the largest difference would then be identified.  I can't provide this answer directly without a coding environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Upload CSV correctly (force mime_type as text/csv)\n",
    "file = genai.upload_file(\n",
    "    path=\"ArtificialData/users.csv\",\n",
    "    mime_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "print(\"Uploaded file:\", file.display_name)\n",
    "print(\"File URI:\", file.uri)\n",
    "\n",
    "# Query Gemini with the uploaded file\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"Answer questions based on this dataset of users:\",\n",
    "    file\n",
    "])\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bd8707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supervisor user ID is `user003` and the admin user ID is `user002`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"User ID of supervisor and admin users in the dataset:\",\n",
    "    file\n",
    "])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0de7cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided dataset only contains user information (userId, lastLogin, createdAt, role).  There is no information about trains, their service status, or assigned job cards.  Therefore, it's impossible to answer your question using this data.  You need a dataset that includes train-related information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"Trains which are out of service and trains which have a job card assigned to them in the dataset:\",\n",
    "    file\n",
    "])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating content: 403 You do not have permission to access the File ogvi0dttsiiq or it may not exist.\n",
      "Note: Could not clean up uploaded files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_folder = \"ArtificialData\"\n",
    "csv_files = [os.path.join(csv_folder, f) for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Upload all CSV files\n",
    "files = [genai.upload_file(path=f, mime_type=\"text/csv\") for f in csv_files]\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"Which Train is due cleaning and which Train is out of service in the dataset:?\",\n",
    "    *files  # unpack list of files\n",
    "])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini AI Prompt Template for Metro CSV-based Q&A\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant that answers metro-related questions based on CSV files from the ArtificialData folder.\n",
    "\n",
    "Follow this JSON schema strictly in your output:\n",
    "\n",
    "{\n",
    "    \"csv_file_path\": \"Path to the CSV file in ArtificialData that contains the data to answer the question.\",\n",
    "    \"question\": \"The user-defined metro-related question to answer based on the CSV data.\",\n",
    "    \"answer\": \"Plain English explanation or answer generated from the CSV data.\",\n",
    "    \"question_type\": \"Type of question based on the CSV content. One of ['summary', 'filtering', 'aggregation', 'transformation', 'comparison'].\",\n",
    "    \"filters\": {\n",
    "        \"column_name\": \"Optional: Name of the column to filter.\",\n",
    "        \"value\": \"Optional: Value to filter the column by.\"\n",
    "    },\n",
    "    \"pandas_code\": \"Suggested pandas code to fetch the answer from the CSV file.\"\n",
    "}\n",
    "\n",
    "Instructions:\n",
    "1. Read the CSV file specified in 'csv_file_path' from the ArtificialData folder.\n",
    "2. Understand the user's 'question'.\n",
    "3. Provide a clear 'answer' in plain English.\n",
    "4. Specify the 'question_type'.\n",
    "5. If needed, suggest filters in 'filters'.\n",
    "6. Provide working pandas code in 'pandas_code' to fetch the answer from the CSV.\n",
    "\n",
    "Example output:\n",
    "\n",
    "{\n",
    "    \"csv_file_path\": \"ArtificialData/trainsets.csv\",\n",
    "    \"question\": \"Which trains are currently out of service?\",\n",
    "    \"answer\": \"Trainsets TS-01 and TS-03 are currently out of service.\",\n",
    "    \"question_type\": \"filtering\",\n",
    "    \"filters\": {\n",
    "        \"column_name\": \"service_status\",\n",
    "        \"value\": \"out of service\"\n",
    "    },\n",
    "    \"pandas_code\": \"import pandas as pd\\ndf = pd.read_csv('ArtificialData/trainsets.csv')\\nout_of_service = df[df['service_status'] == 'out of service']['trainset_id'].tolist()\\nout_of_service\"\n",
    "}\n",
    "\n",
    "Now, generate the JSON output for the user's question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d7403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated structured output:\n",
      "{\n",
      "  \"csv_file_path\": \"train_dataset.csv\",\n",
      "  \"question\": \"Which Train is due cleaning and which Train is out of service in the dataset?\",\n",
      "  \"answer\": \"To answer this question, we need to analyze the 'status' or 'maintenance_status' column of the train dataset.  Trains with a status of 'due_cleaning' are due for cleaning, and trains with a status of 'out_of_service' are out of service.  The specific train IDs will depend on the data in the dataset.\",\n",
      "  \"question_type\": \"filtering\",\n",
      "  \"filters\": {\n",
      "    \"column_name\": \"status\",\n",
      "    \"value\": \"due_cleaning,out_of_service\"\n",
      "  },\n",
      "  \"pandas_code\": \"import pandas as pd\\ndata = pd.read_csv('train_dataset.csv')\\ndue_for_cleaning = data[data['status'].isin(['due_cleaning'])]\\nout_of_service = data[data['status'].isin(['out_of_service'])]\\nprint('Trains due for cleaning:\\\\n', due_for_cleaning)\\nprint('\\\\nTrains out of service:\\\\n', out_of_service)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key (you'll need to set this)\n",
    "# genai.configure(api_key=\"YOUR_API_KEY_HERE\")\n",
    "\n",
    "# Define JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"csv_file_path\": {\"type\": \"string\"},\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "        \"answer\": {\"type\": \"string\"},\n",
    "        \"question_type\": {\"type\": \"string\", \"enum\": [\"summary\", \"filtering\", \"aggregation\", \"transformation\", \"comparison\"]},\n",
    "        \"filters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"column_name\": {\"type\": \"string\"},\n",
    "                \"value\": {\"type\": \"string\"}\n",
    "            }\n",
    "        },\n",
    "        \"pandas_code\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"csv_file_path\", \"question\", \"answer\", \"question_type\"]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Create a more detailed prompt for better results\n",
    "prompt = \"\"\"\n",
    "Please analyze the following question about a train dataset and provide a structured JSON response:\n",
    "\n",
    "Question: \"Which Train is due cleaning and which Train is out of service in the dataset?\"\n",
    "\n",
    "Please provide the response in the following JSON format:\n",
    "{\n",
    "    \"csv_file_path\": \"path/to/your/dataset.csv\",\n",
    "    \"question\": \"Which Train is due cleaning and which Train is out of service in the dataset?\",\n",
    "    \"answer\": \"Your analysis of trains due for cleaning and out of service\",\n",
    "    \"question_type\": \"filtering\",\n",
    "    \"filters\": {\n",
    "        \"column_name\": \"status or maintenance_status\",\n",
    "        \"value\": \"due_cleaning or out_of_service\"\n",
    "    },\n",
    "    \"pandas_code\": \"Python pandas code to filter the data\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Generate structured JSON content\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Safely parse JSON output\n",
    "    try:\n",
    "        structured_output = json.loads(response.text)\n",
    "        print(\"Successfully generated structured output:\")\n",
    "        print(json.dumps(structured_output, indent=2))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: The model did not return valid JSON. Error: {e}\")\n",
    "        print(\"Raw response:\", response.text)\n",
    "        structured_output = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error generating content: {e}\")\n",
    "    structured_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7193955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing file references...\n",
      "Successfully loaded 6 existing uploaded files\n",
      "\n",
      "============================================================\n",
      "RUNNING ANALYSIS...\n",
      "============================================================\n",
      "\n",
      "STRUCTURED JSON OUTPUT:\n",
      "{\n",
      "  \"csv_files_analyzed\": [\n",
      "    \"train_data.csv\",\n",
      "    \"cleaning_schedule.csv\",\n",
      "    \"train_status.csv\"\n",
      "  ],\n",
      "  \"question\": \"Which Train is due cleaning and which Train is out of service in the dataset?\",\n",
      "  \"answer\": \"Trains due for cleaning: TS004, TS005, TS007, TS008, TS013, TS014, TS015, TS021, TS022, TS025. Trains out of service: TS010, TS020\",\n",
      "  \"question_type\": \"filtering\",\n",
      "  \"summary\": {\n",
      "    \"total_files_analyzed\": 3,\n",
      "    \"total_trains_due_cleaning\": 10,\n",
      "    \"total_trains_out_of_service\": 2\n",
      "  },\n",
      "  \"trains_due_cleaning\": [\n",
      "    {\n",
      "      \"train_id\": \"TS004\",\n",
      "      \"train_name\": \"Hyundai\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS005\",\n",
      "      \"train_name\": \"Paytm\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS007\",\n",
      "      \"train_name\": \"Pepsi\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS008\",\n",
      "      \"train_name\": \"Hyundai\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS013\",\n",
      "      \"train_name\": \"ICICI\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS014\",\n",
      "      \"train_name\": \"Pepsi\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS015\",\n",
      "      \"train_name\": \"CocaCola\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS021\",\n",
      "      \"train_name\": \"Mahindra\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS022\",\n",
      "      \"train_name\": \"Airtel\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS025\",\n",
      "      \"train_name\": \"Flipkart\"\n",
      "    }\n",
      "  ],\n",
      "  \"trains_out_of_service\": [\n",
      "    {\n",
      "      \"train_id\": \"TS010\",\n",
      "      \"train_name\": \"McDonald\"\n",
      "    },\n",
      "    {\n",
      "      \"train_id\": \"TS020\",\n",
      "      \"train_name\": \"Reliance\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Files remain uploaded for future use!\n",
      "Run this script again to reuse the same files.\n",
      "To clean up files, uncomment the line below:\n",
      "============================================================\n",
      "\n",
      "Currently uploaded files (32):\n",
      "- users.csv (ID: files/xhucpiaub75x)\n",
      "- trainsets.csv (ID: files/0oj7763bqrcb)\n",
      "- stabling_bays.csv (ID: files/7bx2xmg4krv1)\n",
      "- health_and_maintenance.csv (ID: files/uvxzfk7nf3kw)\n",
      "- cleaning_slots.csv (ID: files/aj4w642cgw1o)\n",
      "- branding_priorities.csv (ID: files/nbhtf7qybgb7)\n",
      "- users.csv (ID: files/imy0vw928di1)\n",
      "- trainsets.csv (ID: files/clf1s362h6at)\n",
      "- stabling_bays.csv (ID: files/t2n7dk9yxz5s)\n",
      "- health_and_maintenance.csv (ID: files/4g1iuy695tmh)\n",
      "- cleaning_slots.csv (ID: files/eacjz9nxys1m)\n",
      "- branding_priorities.csv (ID: files/sbpujgi0t9dv)\n",
      "- users.csv (ID: files/h4nea4p6cphe)\n",
      "- trainsets.csv (ID: files/9jpyj78xmd0x)\n",
      "- stabling_bays.csv (ID: files/3tfne1fgvwin)\n",
      "- health_and_maintenance.csv (ID: files/assb7pl4htmj)\n",
      "- cleaning_slots.csv (ID: files/41ck1raygkib)\n",
      "- branding_priorities.csv (ID: files/1suz4rzsxpw1)\n",
      "- users.csv (ID: files/po14yzslsb35)\n",
      "- trainsets.csv (ID: files/seuam4hyahch)\n",
      "- stabling_bays.csv (ID: files/mb2klwf3aliw)\n",
      "- health_and_maintenance.csv (ID: files/7gu0yor59jw2)\n",
      "- cleaning_slots.csv (ID: files/c1clwv6xy00f)\n",
      "- branding_priorities.csv (ID: files/dmn3vr17yn0b)\n",
      "- users.csv (ID: files/bs3a1jww9c9l)\n",
      "- trainsets.csv (ID: files/t0qqgo192g7b)\n",
      "- stabling_bays.csv (ID: files/143kntz9kl5b)\n",
      "- health_and_maintenance.csv (ID: files/vecw2xt4y4yi)\n",
      "- cleaning_slots.csv (ID: files/0xdyo0hgqng0)\n",
      "- branding_priorities.csv (ID: files/p5x28gdxy1lo)\n",
      "- users.csv (ID: files/us7e1ggb454x)\n",
      "- users.csv (ID: files/bnlybjiywznf)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Define JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"csv_files_analyzed\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "        \"answer\": {\"type\": \"string\"},\n",
    "        \"question_type\": {\"type\": \"string\", \"enum\": [\"summary\", \"filtering\", \"aggregation\", \"transformation\", \"comparison\"]},\n",
    "        \"trains_due_cleaning\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"train_id\": {\"type\": \"string\"},\n",
    "                    \"train_name\": {\"type\": \"string\"},\n",
    "                    \"status\": {\"type\": \"string\"},\n",
    "                    \"last_cleaned\": {\"type\": \"string\"},\n",
    "                    \"source_file\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"trains_out_of_service\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"train_id\": {\"type\": \"string\"},\n",
    "                    \"train_name\": {\"type\": \"string\"},\n",
    "                    \"status\": {\"type\": \"string\"},\n",
    "                    \"reason\": {\"type\": \"string\"},\n",
    "                    \"source_file\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"total_trains_due_cleaning\": {\"type\": \"integer\"},\n",
    "                \"total_trains_out_of_service\": {\"type\": \"integer\"},\n",
    "                \"total_files_analyzed\": {\"type\": \"integer\"}\n",
    "            }\n",
    "        },\n",
    "        \"pandas_code\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"csv_files_analyzed\", \"question\", \"answer\", \"question_type\", \"summary\"]\n",
    "}\n",
    "\n",
    "def upload_csv_files_once(csv_folder=\"ArtificialData\", file_refs_path=\"uploaded_file_refs.json\"):\n",
    "    \"\"\"Upload CSV files once and save their references\"\"\"\n",
    "    \n",
    "    # Check if we already have uploaded file references\n",
    "    if os.path.exists(file_refs_path):\n",
    "        print(\"Loading existing file references...\")\n",
    "        with open(file_refs_path, 'r') as f:\n",
    "            file_data = json.load(f)\n",
    "        \n",
    "        # Verify files still exist on Google's servers\n",
    "        try:\n",
    "            files = []\n",
    "            for file_info in file_data:\n",
    "                file_obj = genai.get_file(file_info['name'])\n",
    "                files.append(file_obj)\n",
    "            print(f\"Successfully loaded {len(files)} existing uploaded files\")\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"Some files may have expired or been deleted: {e}\")\n",
    "            print(\"Re-uploading files...\")\n",
    "    \n",
    "    # Upload files for the first time or re-upload if needed\n",
    "    csv_files = [os.path.join(csv_folder, f) for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {csv_folder}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Uploading {len(csv_files)} CSV files...\")\n",
    "    files = []\n",
    "    file_refs = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            uploaded_file = genai.upload_file(path=csv_file, mime_type=\"text/csv\")\n",
    "            files.append(uploaded_file)\n",
    "            file_refs.append({\n",
    "                'name': uploaded_file.name,\n",
    "                'display_name': uploaded_file.display_name,\n",
    "                'local_path': csv_file\n",
    "            })\n",
    "            print(f\"Uploaded: {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {csv_file}: {e}\")\n",
    "    \n",
    "    # Save file references for future use\n",
    "    with open(file_refs_path, 'w') as f:\n",
    "        json.dump(file_refs, f, indent=2)\n",
    "    \n",
    "    print(f\"Successfully uploaded and saved references for {len(files)} files\")\n",
    "    return files\n",
    "\n",
    "def analyze_trains(files, question=\"Which Train is due cleaning and which Train is out of service in the dataset?\"):\n",
    "    \"\"\"Analyze the uploaded CSV files with the given question\"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the uploaded CSV datasets to answer: \"{question}\"\n",
    "\n",
    "    Please examine all the CSV files and provide a structured JSON response with the following information:\n",
    "    1. List train IDs/names that are due for cleaning (simple array of strings)\n",
    "    2. List train IDs/names that are out of service (simple array of strings)\n",
    "    3. Provide detailed information for each train in separate \"train_details\" section\n",
    "    4. Provide a summary count\n",
    "\n",
    "    Structure your response as follows:\n",
    "    - trains_due_cleaning: Simple array of train identifiers\n",
    "    - trains_out_of_service: Simple array of train identifiers  \n",
    "    - train_details: Separate section with detailed information for each train\n",
    "    - summary: Count totals\n",
    "\n",
    "    Please respond ONLY with valid JSON that matches the specified schema. Do not include any markdown formatting or additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            [prompt, *files],\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=schema\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        structured_output = json.loads(response.text)\n",
    "        return structured_output\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON response. Error: {e}\")\n",
    "        print(\"Raw response:\", response.text)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_uploaded_files():\n",
    "    \"\"\"List all currently uploaded files\"\"\"\n",
    "    try:\n",
    "        files = list(genai.list_files())\n",
    "        print(f\"\\nCurrently uploaded files ({len(files)}):\")\n",
    "        for file in files:\n",
    "            print(f\"- {file.display_name} (ID: {file.name})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "\n",
    "def cleanup_files(file_refs_path=\"uploaded_file_refs.json\"):\n",
    "    \"\"\"Clean up uploaded files and remove references\"\"\"\n",
    "    if os.path.exists(file_refs_path):\n",
    "        with open(file_refs_path, 'r') as f:\n",
    "            file_data = json.load(f)\n",
    "        \n",
    "        for file_info in file_data:\n",
    "            try:\n",
    "                genai.delete_file(file_info['name'])\n",
    "                print(f\"Deleted: {file_info['display_name']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not delete {file_info['display_name']}: {e}\")\n",
    "        \n",
    "        os.remove(file_refs_path)\n",
    "        print(\"Cleaned up file references\")\n",
    "    else:\n",
    "        print(\"No file references found to clean up\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Upload files once (or load existing references)\n",
    "    files = upload_csv_files_once()\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files available for analysis\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Now you can run multiple analyses without re-uploading\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING ANALYSIS...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analysis 1\n",
    "    result = analyze_trains(files)\n",
    "    if result:\n",
    "        print(\"\\nSTRUCTURED JSON OUTPUT:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Save result\n",
    "        with open(\"train_analysis_result.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "    \n",
    "    # You can run more analyses here without re-uploading:\n",
    "    # result2 = analyze_trains(files, \"What is the average age of trains in the dataset?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Files remain uploaded for future use!\")\n",
    "    print(\"Run this script again to reuse the same files.\")\n",
    "    print(\"To clean up files, uncomment the line below:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Uncomment to clean up files when done:\n",
    "    # cleanup_files()\n",
    "    \n",
    "    # List currently uploaded files\n",
    "    list_uploaded_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
